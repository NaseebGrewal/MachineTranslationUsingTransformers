{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Legion 5 Pro 007\\Documents\\Github\\MachineTranslationUsingTransformers\\MachineTranslationUsingTransformers\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\Legion 5 Pro 007\\Documents\\Github\\MachineTranslationUsingTransformers\\MachineTranslationUsingTransformers\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:5109: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Legion 5 Pro 007\\Documents\\Github\\MachineTranslationUsingTransformers\\MachineTranslationUsingTransformers\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 32, 10000])\n"
     ]
    }
   ],
   "source": [
    "import torch  \n",
    "import torch.nn as nn  \n",
    "import torch.optim as optim  \n",
    "import math  \n",
    "  \n",
    "# Setting up the device for GPU usage  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
    "  \n",
    "# Transformer model parameters  \n",
    "d_model = 512  # The number of expected features in the encoder/decoder inputs  \n",
    "nhead = 8  # The number of heads in the multiheadattention models  \n",
    "num_encoder_layers = 3  # The number of sub-encoder-layers in the encoder  \n",
    "num_decoder_layers = 3  # The number of sub-decoder-layers in the decoder  \n",
    "dim_feedforward = 2048  # The dimension of the feedforward network model  \n",
    "dropout = 0.1  # The dropout value  \n",
    "  \n",
    "# Sample tokenizers (these should be replaced with the actual tokenizers for your languages)  \n",
    "src_language = 'en'  \n",
    "tgt_language = 'fr'  \n",
    "  \n",
    "# Replace these with the actual vocabulary sizes for your source and target languages  \n",
    "src_vocab_size = 10000  \n",
    "tgt_vocab_size = 10000  \n",
    "  \n",
    "class TransformerModel(nn.Module):  \n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout):  \n",
    "        super(TransformerModel, self).__init__()  \n",
    "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_encoder_layers,  \n",
    "                                          num_decoder_layers=num_decoder_layers, dim_feedforward=dim_feedforward, dropout=dropout)  \n",
    "        self.src_tok_emb = nn.Embedding(src_vocab_size, d_model)  \n",
    "        self.tgt_tok_emb = nn.Embedding(tgt_vocab_size, d_model)  \n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)  \n",
    "        self.generator = nn.Linear(d_model, tgt_vocab_size)  \n",
    "  \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):  \n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))  \n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))  \n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None, src_padding_mask, tgt_padding_mask, memory_key_padding_mask)  \n",
    "        return self.generator(outs)  \n",
    "  \n",
    "class PositionalEncoding(nn.Module):  \n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):  \n",
    "        super(PositionalEncoding, self).__init__()  \n",
    "        self.dropout = nn.Dropout(p=dropout)  \n",
    "  \n",
    "        position = torch.arange(max_len).unsqueeze(1)  \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))  \n",
    "        pe = torch.zeros(max_len, 1, d_model)  \n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)  \n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)  \n",
    "        self.register_buffer('pe', pe)  \n",
    "  \n",
    "    def forward(self, x):  \n",
    "        x = x + self.pe[:x.size(0)]  \n",
    "        return self.dropout(x)  \n",
    "  \n",
    "# Instantiate the model  \n",
    "transformer_model = TransformerModel(src_vocab_size, tgt_vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout).to(device)  \n",
    "  \n",
    "# Example input batch  \n",
    "src = torch.rand((10, 32)).long().to(device)  # (source sequence length, batch size)  \n",
    "tgt = torch.rand((20, 32)).long().to(device)  # (target sequence length, batch size)  \n",
    "  \n",
    "# Masks and padding  \n",
    "src_mask = transformer_model.transformer.generate_square_subsequent_mask(src.size(0)).to(device)  \n",
    "tgt_mask = transformer_model.transformer.generate_square_subsequent_mask(tgt.size(0)).to(device)  \n",
    "src_padding_mask = (src == 0).transpose(0, 1).to(device)  \n",
    "tgt_padding_mask = (tgt == 0).transpose(0, 1).to(device)  \n",
    "memory_key_padding_mask = src_padding_mask  \n",
    "  \n",
    "# Forward pass  \n",
    "outputs = transformer_model(src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask)  \n",
    "  \n",
    "print(outputs.shape)  # (target sequence length, batch size, target vocabulary size)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformersNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: c:\\Users\\Legion 5 Pro 007\\Documents\\Github\\MachineTranslationUsingTransformers\\MachineTranslationUsingTransformers\\.venv\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/e2/52/02271ef16713abea41bab736dfc2dbee75e5e3512cf7441e233976211ba5/transformers-4.39.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.39.2-py3-none-any.whl.metadata (134 kB)\n",
      "     ---------------------------------------- 0.0/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     ----------------------- --------------- 81.9/134.8 kB 1.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 134.8/134.8 kB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\legion 5 pro 007\\documents\\github\\machinetranslationusingtransformers\\machinetranslationusingtransformers\\.venv\\lib\\site-packages (from transformers) (3.13.3)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.19.3 from https://files.pythonhosted.org/packages/05/c0/779afbad8e75565c09ffa24a88b5dd7e293c92b74eb09df6435fc58ac986/huggingface_hub-0.22.2-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\legion 5 pro 007\\documents\\github\\machinetranslationusingtransformers\\machinetranslationusingtransformers\\.venv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\legion 5 pro 007\\documents\\github\\machinetranslationusingtransformers\\machinetranslationusingtransformers\\.venv\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\legion 5 pro 007\\documents\\github\\machinetranslationusingtransformers\\machinetranslationusingtransformers\\.venv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\legion 5 pro 007\\documents\\github\\machinetranslationusingtransformers\\machinetranslationusingtransformers\\.venv\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\legion 5 pro 007\\documents\\github\\machinetranslationusingtransformers\\machinetranslationusingtransformers\\.venv\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/c1/02/40725eebedea8175918bd59ab80b2174d6ef3b3ef9ac8ec996e84c38d3ca/tokenizers-0.15.2-cp311-none-win_amd64.whl.metadata\n",
      "  Downloading tokenizers-0.15.2-cp311-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.4.1 from https://files.pythonhosted.org/packages/54/ce/09e508c682c612e7ff7b5cf1249a963d10db58f16d77007177f7770c661b/safetensors-0.4.2-cp311-none-win_amd64.whl.metadata\n",
      "  Downloading safetensors-0.4.2-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\legion 5 pro 007\\documents\\github\\machinetranslationusingtransformers\\machinetranslationusingtransformers\\.venv\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\legion 5 pro 007\\documents\\github\\machinetranslationusingtransformers\\machinetranslationusingtransformers\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\legion 5 pro 007\\documents\\github\\machinetranslationusingtransformers\\machinetranslationusingtransformers\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\legion 5 pro 007\\documents\\github\\machinetranslationusingtransformers\\machinetranslationusingtransformers\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\legion 5 pro 007\\documents\\github\\machinetranslationusingtransformers\\machinetranslationusingtransformers\\.venv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\legion 5 pro 007\\documents\\github\\machinetranslationusingtransformers\\machinetranslationusingtransformers\\.venv\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\legion 5 pro 007\\documents\\github\\machinetranslationusingtransformers\\machinetranslationusingtransformers\\.venv\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\legion 5 pro 007\\documents\\github\\machinetranslationusingtransformers\\machinetranslationusingtransformers\\.venv\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading transformers-4.39.2-py3-none-any.whl (8.8 MB)\n",
      "   ---------------------------------------- 0.0/8.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.2/8.8 MB 6.9 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.9/8.8 MB 14.0 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 2.3/8.8 MB 20.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 4.5/8.8 MB 32.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.8/8.8 MB 39.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.8/8.8 MB 43.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.8/8.8 MB 43.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.8/8.8 MB 33.1 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "   ---------------------------------------- 0.0/388.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 388.9/388.9 kB 23.7 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.2-cp311-none-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 269.6/269.6 kB ? eta 0:00:00\n",
      "Downloading tokenizers-0.15.2-cp311-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 145.5 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.22.2 safetensors-0.4.2 tokenizers-0.15.2 transformers-4.39.2\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-base and revision 686f1db (https://huggingface.co/google-t5/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "c:\\Users\\Legion 5 Pro 007\\Documents\\Github\\MachineTranslationUsingTransformers\\MachineTranslationUsingTransformers\\.venv\\Lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:171: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on google-t5/t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "french translation: Comment le temps est-il aujourd'hui\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline  \n",
    "  \n",
    "# Initialize the translation pipeline  \n",
    "translator = pipeline(\"translation_en_to_fr\")  \n",
    "  \n",
    "# Function to translate English to French  \n",
    "def translate_to_french(text):  \n",
    "    translation = translator(text)  \n",
    "    return translation[0]['translation_text']  \n",
    "  \n",
    "# User input  \n",
    "english_text = input(\"Enter text in English to translate to French: \")  \n",
    "# english_text = \"how are you\"\n",
    "\n",
    "\n",
    "# Translate and print the result  \n",
    "french_translation = translate_to_french(english_text)  \n",
    "print(f\"french translation: {french_translation}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\legion 5 pro 007\\documents\\github\\machinetranslationusingtransformers\\machinetranslationusingtransformers\\.venv\\lib\\site-packages (4.39.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting sentencepiece\n",
      "  Obtaining dependency information for sentencepiece from https://files.pythonhosted.org/packages/a2/f6/587c62fd21fc988555b85351f50bbde43a51524caafd63bc69240ded14fd/sentencepiece-0.2.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\legion 5 pro 007\\documents\\github\\machinetranslationusingtransformers\\machinetranslationusingtransformers\\.venv\\lib\\site-packages (from transformers) (3.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\legion 5 pro 007\\documents\\github\\machinetranslationusingtransformers\\machinetranslationusingtransformers\\.venv\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\legion 5 pro 007\\documents\\github\\machinetranslationusingtransformers\\machinetranslationusingtransformers\\.venv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\legion 5 pro 007\\documents\\github\\machinetranslationusingtransformers\\machinetranslationusingtransformers\\.venv\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\legion 5 pro 007\\documents\\github\\machinetranslationusingtransformers\\machinetranslationusingtransformers\\.venv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\legion 5 pro 007\\documents\\github\\machinetranslationusingtransformers\\machinetranslationusingtransformers\\.venv\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\legion 5 pro 007\\documents\\github\\machinetranslationusingtransformers\\machinetranslationusingtransformers\\.venv\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\legion 5 pro 007\\documents\\github\\machinetranslationusingtransformers\\machinetranslationusingtransformers\\.venv\\lib\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\legion 5 pro 007\\documents\\github\\machinetranslationusingtransformers\\machinetranslationusingtransformers\\.venv\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\legion 5 pro 007\\documents\\github\\machinetranslationusingtransformers\\machinetranslationusingtransformers\\.venv\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\legion 5 pro 007\\documents\\github\\machinetranslationusingtransformers\\machinetranslationusingtransformers\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\legion 5 pro 007\\documents\\github\\machinetranslationusingtransformers\\machinetranslationusingtransformers\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\legion 5 pro 007\\documents\\github\\machinetranslationusingtransformers\\machinetranslationusingtransformers\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\legion 5 pro 007\\documents\\github\\machinetranslationusingtransformers\\machinetranslationusingtransformers\\.venv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\legion 5 pro 007\\documents\\github\\machinetranslationusingtransformers\\machinetranslationusingtransformers\\.venv\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\legion 5 pro 007\\documents\\github\\machinetranslationusingtransformers\\machinetranslationusingtransformers\\.venv\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\legion 5 pro 007\\documents\\github\\machinetranslationusingtransformers\\machinetranslationusingtransformers\\.venv\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   - -------------------------------------- 30.7/991.5 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 256.0/991.5 kB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 778.2/991.5 kB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 991.5/991.5 kB 7.9 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: c:\\Users\\Legion 5 Pro 007\\Documents\\Github\\MachineTranslationUsingTransformers\\MachineTranslationUsingTransformers\\.venv\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers sentencepiece  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab0d3b912c4451ba2af08ae4803bec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Legion 5 Pro 007\\Documents\\Github\\MachineTranslationUsingTransformers\\MachineTranslationUsingTransformers\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Legion 5 Pro 007\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-de. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e67c11c0b44d3eafc25ca66d0a40e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/768k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22d03dd91fd4b018fbb67d4d70cdbb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/797k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee4ed8d50054417bf2a0bb8b5a05261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51a02a5f824d45f782b2e8f709c0d60b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Legion 5 Pro 007\\Documents\\Github\\MachineTranslationUsingTransformers\\MachineTranslationUsingTransformers\\.venv\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4fbcaaefdf401cbecb21e9e34d3217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/298M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f44ad82a136c4a63a43af838ce0b110c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text (de): wie ist dein Name?\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer  \n",
    "  \n",
    "def translate_text(text, target_language):  \n",
    "    # Define the model repository path  \n",
    "    model_name = f'Helsinki-NLP/opus-mt-en-{target_language}'  \n",
    "      \n",
    "    # Load the tokenizer and model  \n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)  \n",
    "    model = MarianMTModel.from_pretrained(model_name)  \n",
    "  \n",
    "    # Tokenize the text  \n",
    "    translated = model.generate(**tokenizer(text, return_tensors=\"pt\", padding=True))  \n",
    "  \n",
    "    # Decode the tokens to string  \n",
    "    translation = tokenizer.decode(translated[0], skip_special_tokens=True)  \n",
    "  \n",
    "    return translation  \n",
    "  \n",
    "# Example usage:  \n",
    "if __name__ == \"__main__\":  \n",
    "    # User input  \n",
    "    english_text = input(\"Enter English text to translate: \")  \n",
    "    target_language = input(\"Enter target language code (e.g., 'fr' for French): \")  \n",
    "  \n",
    "    # Translate the text  \n",
    "    translated_text = translate_text(english_text, target_language)  \n",
    "  \n",
    "    # Output the translation  \n",
    "    print(f\"Translated text ({target_language}): {translated_text}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "471b5e842ff64513ab2ad6336b13d643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Legion 5 Pro 007\\Documents\\Github\\MachineTranslationUsingTransformers\\MachineTranslationUsingTransformers\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Legion 5 Pro 007\\.cache\\huggingface\\hub\\models--sshleifer--distilbart-cnn-12-6. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d232a62d2945bb9ddfb72afb6f345f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01750b7c918f429498b5a10ea4ad3ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82609301cf1c4a00849ed85cd92ce09b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a58066579bf4f2c9983fb596ff4171c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "  The sanitize function can be extended to perform additional sanitization tasks, depending on what kind of input you are expecting . For example, escaping HTML is necessary when inserting data into an HTML template . For database queries, using parameterized queries or the ORM's built-in methods is generally sufficient to prevent injection attacks .\n"
     ]
    }
   ],
   "source": [
    "#Summerization\n",
    "\n",
    "from transformers import pipeline  \n",
    "  \n",
    "# Load the summarization pipeline  \n",
    "summarizer = pipeline(\"summarization\")  \n",
    "  \n",
    "def get_summary(text):  \n",
    "    # Use the model to generate a summary  \n",
    "    summary = summarizer(text, max_length=130, min_length=30, do_sample=False)  \n",
    "    return summary[0]['summary_text']  \n",
    "  \n",
    "# Get input from the user  \n",
    "user_input = input(\"Please type the English text you want to summarize:\\n\")  \n",
    "  \n",
    "# Check if the text is too short to summarize  \n",
    "if len(user_input.split()) < 56:  \n",
    "    print(\"This text is too short to summarize, please provide more content.\")  \n",
    "else:  \n",
    "    # Call the get_summary function  \n",
    "    summary = get_summary(user_input)  \n",
    "    print(\"\\nSummary:\\n\", summary)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
