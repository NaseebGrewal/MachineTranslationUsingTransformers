{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  \n",
    "import torch.nn as nn  \n",
    "import torch.optim as optim  \n",
    "import math  \n",
    "  \n",
    "# Setting up the device for GPU usage  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
    "  \n",
    "# Transformer model parameters  \n",
    "d_model = 512  # The number of expected features in the encoder/decoder inputs  \n",
    "nhead = 8  # The number of heads in the multiheadattention models  \n",
    "num_encoder_layers = 3  # The number of sub-encoder-layers in the encoder  \n",
    "num_decoder_layers = 3  # The number of sub-decoder-layers in the decoder  \n",
    "dim_feedforward = 2048  # The dimension of the feedforward network model  \n",
    "dropout = 0.1  # The dropout value  \n",
    "  \n",
    "# Sample tokenizers (these should be replaced with the actual tokenizers for your languages)  \n",
    "src_language = 'en'  \n",
    "tgt_language = 'fr'  \n",
    "  \n",
    "# Replace these with the actual vocabulary sizes for your source and target languages  \n",
    "src_vocab_size = 10000  \n",
    "tgt_vocab_size = 10000  \n",
    "  \n",
    "class TransformerModel(nn.Module):  \n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout):  \n",
    "        super(TransformerModel, self).__init__()  \n",
    "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_encoder_layers,  \n",
    "                                          num_decoder_layers=num_decoder_layers, dim_feedforward=dim_feedforward, dropout=dropout)  \n",
    "        self.src_tok_emb = nn.Embedding(src_vocab_size, d_model)  \n",
    "        self.tgt_tok_emb = nn.Embedding(tgt_vocab_size, d_model)  \n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)  \n",
    "        self.generator = nn.Linear(d_model, tgt_vocab_size)  \n",
    "  \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):  \n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))  \n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))  \n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None, src_padding_mask, tgt_padding_mask, memory_key_padding_mask)  \n",
    "        return self.generator(outs)  \n",
    "  \n",
    "class PositionalEncoding(nn.Module):  \n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):  \n",
    "        super(PositionalEncoding, self).__init__()  \n",
    "        self.dropout = nn.Dropout(p=dropout)  \n",
    "  \n",
    "        position = torch.arange(max_len).unsqueeze(1)  \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))  \n",
    "        pe = torch.zeros(max_len, 1, d_model)  \n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)  \n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)  \n",
    "        self.register_buffer('pe', pe)  \n",
    "  \n",
    "    def forward(self, x):  \n",
    "        x = x + self.pe[:x.size(0)]  \n",
    "        return self.dropout(x)  \n",
    "  \n",
    "# Instantiate the model  \n",
    "transformer_model = TransformerModel(src_vocab_size, tgt_vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout).to(device)  \n",
    "  \n",
    "# Example input batch  \n",
    "src = torch.rand((10, 32)).long().to(device)  # (source sequence length, batch size)  \n",
    "tgt = torch.rand((20, 32)).long().to(device)  # (target sequence length, batch size)  \n",
    "  \n",
    "# Masks and padding  \n",
    "src_mask = transformer_model.transformer.generate_square_subsequent_mask(src.size(0)).to(device)  \n",
    "tgt_mask = transformer_model.transformer.generate_square_subsequent_mask(tgt.size(0)).to(device)  \n",
    "src_padding_mask = (src == 0).transpose(0, 1).to(device)  \n",
    "tgt_padding_mask = (tgt == 0).transpose(0, 1).to(device)  \n",
    "memory_key_padding_mask = src_padding_mask  \n",
    "  \n",
    "# Forward pass  \n",
    "outputs = transformer_model(src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask)  \n",
    "  \n",
    "print(outputs.shape)  # (target sequence length, batch size, target vocabulary size)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline  \n",
    "  \n",
    "# Initialize the translation pipeline  \n",
    "translator = pipeline(\"translation_en_to_fr\")  \n",
    "  \n",
    "# Function to translate English to French  \n",
    "def translate_to_french(text):  \n",
    "    translation = translator(text)  \n",
    "    return translation[0]['translation_text']  \n",
    "  \n",
    "# User input  \n",
    "english_text = input(\"Enter text in English to translate to French: \")  \n",
    "# english_text = \"how are you\"\n",
    "\n",
    "\n",
    "# Translate and print the result  \n",
    "french_translation = translate_to_french(english_text)  \n",
    "print(f\"french translation: {french_translation}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers sentencepiece  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer  \n",
    "  \n",
    "def translate_text(text, target_language):  \n",
    "    # Define the model repository path  \n",
    "    model_name = f'Helsinki-NLP/opus-mt-en-{target_language}'  \n",
    "      \n",
    "    # Load the tokenizer and model  \n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)  \n",
    "    model = MarianMTModel.from_pretrained(model_name)  \n",
    "  \n",
    "    # Tokenize the text  \n",
    "    translated = model.generate(**tokenizer(text, return_tensors=\"pt\", padding=True))  \n",
    "  \n",
    "    # Decode the tokens to string  \n",
    "    translation = tokenizer.decode(translated[0], skip_special_tokens=True)  \n",
    "  \n",
    "    return translation  \n",
    "  \n",
    "# Example usage:  \n",
    "if __name__ == \"__main__\":  \n",
    "    # User input  \n",
    "    english_text = input(\"Enter English text to translate: \")  \n",
    "    target_language = input(\"Enter target language code (e.g., 'fr' for French): \")  \n",
    "  \n",
    "    # Translate the text  \n",
    "    translated_text = translate_text(english_text, target_language)  \n",
    "  \n",
    "    # Output the translation  \n",
    "    print(f\"Translated text ({target_language}): {translated_text}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summerization\n",
    "\n",
    "from transformers import pipeline  \n",
    "  \n",
    "# Load the summarization pipeline  \n",
    "summarizer = pipeline(\"summarization\")  \n",
    "  \n",
    "def get_summary(text):  \n",
    "    # Use the model to generate a summary  \n",
    "    summary = summarizer(text, max_length=130, min_length=30, do_sample=False)  \n",
    "    return summary[0]['summary_text']  \n",
    "  \n",
    "# Get input from the user  \n",
    "user_input = input(\"Please type the English text you want to summarize:\\n\")  \n",
    "  \n",
    "# Check if the text is too short to summarize  \n",
    "if len(user_input.split()) < 56:  \n",
    "    print(\"This text is too short to summarize, please provide more content.\")  \n",
    "else:  \n",
    "    # Call the get_summary function  \n",
    "    summary = get_summary(user_input)  \n",
    "    print(\"\\nSummary:\\n\", summary)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
